{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGLfQTPXlQP"
      },
      "source": [
        "<center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">به نام خدا</div></center>\n",
        "\n",
        "<h1><center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">رگرسیون تخمین قیمت خانه با چند ورودی<br>تصویر+داده های ساختار یافته\n",
        "</div></center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای اینکه داده ها رو کامل بیارم، این روش و استفاده میکنم"
      ],
      "metadata": {
        "id": "YwQUj6uCXnm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset.git"
      ],
      "metadata": {
        "id": "TaT_yAVyXpE4",
        "outputId": "fec1c6df-16dc-48c2-f85e-2fbb49467628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2165\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 40.42 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Updating files: 100% (2144/2144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tKSOEPc1XlQS"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import locale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9-HeHTb3XlQU"
      },
      "outputs": [],
      "source": [
        "inputPath = \"/content/Houses-dataset/Houses Dataset/HousesInfo.txt\"\n",
        "datasetPath = \"/content/Houses-dataset/Houses Dataset\"\n",
        "\n",
        "cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "df = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\n",
        "\n",
        "zipcodes, counts = np.unique(df[\"zipcode\"], return_counts=True)\n",
        "\n",
        "# loop over each of the unique zip codes and their corresponding\n",
        "# count\n",
        "for (zipcode, count) in zip(zipcodes, counts):\n",
        "    # the zip code counts for our housing dataset is *extremely*\n",
        "    # unbalanced (some only having 1 or 2 houses per zip code)\n",
        "    # so let's sanitize our data by removing any houses with less\n",
        "    # than 25 houses per zip code\n",
        "    if count < 25:\n",
        "        idxs = df[df[\"zipcode\"] == zipcode].index\n",
        "        df.drop(idxs, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nvu3DVd0XlQV"
      },
      "outputs": [],
      "source": [
        "# initialize our images array (i.e., the house images themselves)\n",
        "images = []\n",
        "\n",
        "# loop over the indexes of the houses\n",
        "for i in df.index.values:\n",
        "    # find the four images for the house and sort the file paths,\n",
        "    # ensuring the four are always in the *same order*\n",
        "    basePath = os.path.sep.join([datasetPath, \"{}_*\".format(i + 1)])\n",
        "    housePaths = sorted(list(glob.glob(basePath)))\n",
        "    # initialize our list of input images along with the output image\n",
        "    # after *combining* the four input images\n",
        "    inputImages = []\n",
        "    outputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\n",
        "    # loop over the input house paths\n",
        "    for housePath in housePaths:\n",
        "        # load the input image, resize it to be 32 32, and then\n",
        "        # update the list of input images\n",
        "        image = cv2.imread(housePath)\n",
        "        image = cv2.resize(image, (32, 32))\n",
        "        inputImages.append(image)\n",
        "\n",
        "    # tile the four input images in the output image such the first\n",
        "    # image goes in the top-right corner, the second image in the\n",
        "    # top-left corner, the third image in the bottom-right corner,\n",
        "    # and the final image in the bottom-left corner\n",
        "    outputImage[0:32, 0:32] = inputImages[0]\n",
        "    outputImage[0:32, 32:64] = inputImages[1]\n",
        "    outputImage[32:64, 32:64] = inputImages[2]\n",
        "    outputImage[32:64, 0:32] = inputImages[3]\n",
        "\n",
        "    # add the tiled image to our set of images the network will be\n",
        "    # trained on\n",
        "    images.append(outputImage)\n",
        "images = np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5cMJ5ZKiXlQW"
      },
      "outputs": [],
      "source": [
        "images = images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pd5_d-GbXlQW"
      },
      "outputs": [],
      "source": [
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kshbDH0oXlQW"
      },
      "outputs": [],
      "source": [
        "# find the largest house price in the training set and use it to\n",
        "# scale our house prices to the range [0, 1] (will lead to better\n",
        "# training and convergence)\n",
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J4C5tNxxXlQX"
      },
      "outputs": [],
      "source": [
        "# initialize the column names of the continuous data\n",
        "continuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\n",
        "# performin min-max scaling each continuous feature column to\n",
        "# the range [0, 1]\n",
        "cs = MinMaxScaler()\n",
        "trainContinuous = cs.fit_transform(trainAttrX[continuous])\n",
        "testContinuous = cs.transform(testAttrX[continuous])\n",
        "\n",
        "# one-hot encode the zip code categorical data (by definition of\n",
        "# one-hot encoing, all output features are now in the range [0, 1])\n",
        "zipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "trainCategorical = zipBinarizer.transform(trainAttrX[\"zipcode\"])\n",
        "testCategorical = zipBinarizer.transform(testAttrX[\"zipcode\"])\n",
        "\n",
        "# construct our training and testing data points by concatenating\n",
        "# the categorical features with the continuous features\n",
        "trainAttrX = np.hstack([trainCategorical, trainContinuous])\n",
        "testAttrX = np.hstack([testCategorical, testContinuous])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IXcoFL4hXlQX"
      },
      "outputs": [],
      "source": [
        "dim = trainAttrX.shape[1] #10\n",
        "\n",
        "# define our MLP network\n",
        "mlp = Sequential()\n",
        "mlp.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "mlp.add(Dense(4, activation=\"relu\"))\n",
        "\n",
        "\n",
        "### لایه آخر و برمیداریم چون نمیخوایم پردیکت کنیم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mpew6KIZXlQY"
      },
      "outputs": [],
      "source": [
        "from keras import Model, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dense, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "width, height, depth = 64, 64, 3\n",
        "filters=(16, 32, 64)\n",
        "# initialize the input shape and channel dimension, assuming\n",
        "# TensorFlow/channels-last ordering\n",
        "inputShape = (height, width, depth)\n",
        "\n",
        "# define the model input\n",
        "inputs = Input(shape=inputShape)\n",
        "\n",
        "# loop over the number of filters\n",
        "for (i, f) in enumerate(filters):\n",
        "    # if this is the first CONV layer then set the input\n",
        "    # appropriately\n",
        "    if i == 0:\n",
        "        x = inputs\n",
        "\n",
        "    # CONV => RELU => BN => POOL\n",
        "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "x = Flatten()(x)\n",
        "x = Dense(16)(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# apply another FC layer, this one to match the number of nodes\n",
        "# coming out of the MLP\n",
        "x = Dense(4)(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "\n",
        "# construct the CNN\n",
        "cnn = Model(inputs, x)\n",
        "\n",
        "### لایه آخر و برمیداریم چون نمیخوایم پردیکت کنیم"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x9ofYZ1MXlQY"
      },
      "outputs": [],
      "source": [
        "from keras.layers import concatenate\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        "\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MeX7xR_XXlQZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer='adam')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "G3ZCHFvGXlQa",
        "outputId": "e2d00699-e96b-46fd-b3d8-8232ead318b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "34/34 [==============================] - 4s 44ms/step - loss: 428.0497 - val_loss: 71.4020\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 211.5413 - val_loss: 76.9516\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 117.1294 - val_loss: 78.5101\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 99.8255 - val_loss: 78.0143\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 81.3931 - val_loss: 72.5986\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 76.0790 - val_loss: 71.5195\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 71.4254 - val_loss: 71.4151\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 62.3688 - val_loss: 70.9159\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 78.7742 - val_loss: 71.2082\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 56.8235 - val_loss: 70.9452\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 1s 35ms/step - loss: 51.8155 - val_loss: 71.9220\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 58.5127 - val_loss: 70.9157\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 58.4225 - val_loss: 66.8772\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 57.7037 - val_loss: 69.6014\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 49.6186 - val_loss: 60.5533\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 45.0317 - val_loss: 55.8863\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 47.3071 - val_loss: 52.9315\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 47.8936 - val_loss: 46.6351\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 46.7615 - val_loss: 41.6454\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 43.8766 - val_loss: 45.3090\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 40.1880 - val_loss: 48.0366\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 1s 32ms/step - loss: 39.6220 - val_loss: 49.1511\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 40.5406 - val_loss: 42.5903\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 37.5271 - val_loss: 60.4378\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 37.2809 - val_loss: 47.6949\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 39.0499 - val_loss: 46.0970\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 36.7500 - val_loss: 44.1134\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 35.6553 - val_loss: 40.6287\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 35.6696 - val_loss: 37.4736\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 33.3706 - val_loss: 38.9365\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 39.7371 - val_loss: 67.6407\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 41.2076 - val_loss: 66.5697\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 34.9641 - val_loss: 43.1557\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 42.1071 - val_loss: 37.0466\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 38.9092 - val_loss: 36.3141\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 34.0166 - val_loss: 35.8049\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 32.1050 - val_loss: 32.8350\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 34.0100 - val_loss: 32.3554\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 30.9822 - val_loss: 32.1328\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 31.7893 - val_loss: 33.0090\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 30.5327 - val_loss: 32.8488\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 30.0735 - val_loss: 31.5073\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 28.7324 - val_loss: 31.9378\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 30.5213 - val_loss: 30.8629\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 1s 38ms/step - loss: 29.7435 - val_loss: 31.2073\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 1s 35ms/step - loss: 29.4354 - val_loss: 29.8697\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 29.8168 - val_loss: 29.9837\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 30.8163 - val_loss: 175.5688\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 39.6143 - val_loss: 92.9967\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 35.8829 - val_loss: 143.2753\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 30.3963 - val_loss: 324.9432\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 29.0632 - val_loss: 37.2693\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 29.7860 - val_loss: 34.7009\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 26.9839 - val_loss: 29.3619\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 29.0157 - val_loss: 29.2942\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 1s 33ms/step - loss: 27.6620 - val_loss: 30.7209\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 1s 40ms/step - loss: 26.5283 - val_loss: 28.6321\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 25.0885 - val_loss: 31.3395\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 27.1855 - val_loss: 31.0746\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 26.9216 - val_loss: 29.3697\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 26.1224 - val_loss: 30.4085\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 23.8135 - val_loss: 31.5801\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 24.7345 - val_loss: 28.4779\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 25.8378 - val_loss: 30.2622\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 24.8948 - val_loss: 31.0341\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 26.9871 - val_loss: 27.5263\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 1s 33ms/step - loss: 27.0258 - val_loss: 29.1717\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 1s 41ms/step - loss: 24.7103 - val_loss: 30.7472\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 24.7876 - val_loss: 29.0850\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 25.0230 - val_loss: 29.0777\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 24.6481 - val_loss: 29.5141\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 24.4470 - val_loss: 30.6565\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 23.9934 - val_loss: 30.0146\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 21.8720 - val_loss: 29.1132\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 25.6033 - val_loss: 29.1071\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 23.0865 - val_loss: 27.2431\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 24.5155 - val_loss: 26.7354\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 22.9119 - val_loss: 29.4085\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 25.1357 - val_loss: 27.3200\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 24.1728 - val_loss: 27.5185\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.8852 - val_loss: 27.6720\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 24.4582 - val_loss: 90.9143\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.7596 - val_loss: 27.0960\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.4039 - val_loss: 28.8802\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 21.5285 - val_loss: 27.2826\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.5523 - val_loss: 24.7109\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 21.9762 - val_loss: 24.7881\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 19.8334 - val_loss: 26.5057\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 21.4553 - val_loss: 23.3639\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 2s 44ms/step - loss: 19.5800 - val_loss: 25.0719\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.0814 - val_loss: 26.3527\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 21.5690 - val_loss: 22.2799\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 22.0025 - val_loss: 23.3578\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 20.4875 - val_loss: 21.2049\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 20.7735 - val_loss: 32.2183\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 22.3616 - val_loss: 26.4244\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 20.1883 - val_loss: 21.5531\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.6605 - val_loss: 21.1536\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.8619 - val_loss: 20.7636\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.5633 - val_loss: 22.1051\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 2s 45ms/step - loss: 19.9188 - val_loss: 21.7135\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 1s 31ms/step - loss: 18.3281 - val_loss: 23.8802\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.3976 - val_loss: 20.4123\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.5242 - val_loss: 19.7691\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 19.0394 - val_loss: 21.1443\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.7289 - val_loss: 20.1228\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.2893 - val_loss: 21.8421\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.8850 - val_loss: 24.1408\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.6196 - val_loss: 20.8564\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.4510 - val_loss: 21.0108\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 19.4599 - val_loss: 23.6069\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 18.6543 - val_loss: 20.5982\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.1506 - val_loss: 20.2681\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.9317 - val_loss: 20.6081\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.9582 - val_loss: 20.7896\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.0836 - val_loss: 20.7256\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 16.8207 - val_loss: 20.4826\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.9672 - val_loss: 22.7400\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.1733 - val_loss: 21.4006\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.7446 - val_loss: 20.8434\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.7414 - val_loss: 20.4229\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.4748 - val_loss: 20.4637\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 16.7347 - val_loss: 21.6551\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.8007 - val_loss: 19.9734\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 18.5628 - val_loss: 20.2845\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.2345 - val_loss: 20.3800\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.2464 - val_loss: 20.7312\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 18.0856 - val_loss: 22.3563\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.4537 - val_loss: 20.5840\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 16.6466 - val_loss: 19.8758\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.5579 - val_loss: 20.1093\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 19.1809 - val_loss: 19.9701\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.4226 - val_loss: 21.1738\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 1s 44ms/step - loss: 17.4324 - val_loss: 21.2018\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.6942 - val_loss: 21.5200\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.3801 - val_loss: 21.6464\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.2305 - val_loss: 19.5163\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.3670 - val_loss: 20.3727\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.3754 - val_loss: 19.3086\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 15.6291 - val_loss: 20.4428\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.2592 - val_loss: 19.4952\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.4893 - val_loss: 19.6680\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 15.4093 - val_loss: 19.2802\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.8884 - val_loss: 22.6077\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 2s 45ms/step - loss: 17.1952 - val_loss: 21.1079\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.1316 - val_loss: 20.0210\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 16.4347 - val_loss: 20.1043\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.8177 - val_loss: 19.9979\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.4584 - val_loss: 59.9699\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.3254 - val_loss: 24.6629\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.2385 - val_loss: 23.4257\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.5081 - val_loss: 22.8372\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 17.4896 - val_loss: 22.0695\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.3147 - val_loss: 21.3550\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 19.0180 - val_loss: 22.9780\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 17.9732 - val_loss: 21.3219\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 18.1005 - val_loss: 21.1453\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.3502 - val_loss: 19.8592\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.5989 - val_loss: 20.1658\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.0441 - val_loss: 21.4761\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.9620 - val_loss: 25.3278\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 16.4322 - val_loss: 23.1194\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.0132 - val_loss: 21.6644\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.2038 - val_loss: 19.6824\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.5568 - val_loss: 19.5621\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.2178 - val_loss: 19.0343\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 16.1299 - val_loss: 21.4984\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 16.8713 - val_loss: 20.9134\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.4410 - val_loss: 18.8792\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.9247 - val_loss: 19.4078\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.5132 - val_loss: 20.3601\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 18.5264 - val_loss: 24.9721\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.3494 - val_loss: 20.5323\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.1080 - val_loss: 18.9870\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 17.4193 - val_loss: 20.7854\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.3872 - val_loss: 20.0720\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.2238 - val_loss: 20.4919\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 2s 45ms/step - loss: 17.1168 - val_loss: 19.7552\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.6107 - val_loss: 22.9358\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.3948 - val_loss: 20.3704\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.8693 - val_loss: 19.5747\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.0803 - val_loss: 22.4656\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.5942 - val_loss: 21.0307\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.9814 - val_loss: 20.8653\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 15.8991 - val_loss: 19.8920\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.2757 - val_loss: 19.1281\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.4535 - val_loss: 20.7713\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.2961 - val_loss: 19.2746\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 1s 43ms/step - loss: 17.9002 - val_loss: 26.3389\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 1s 31ms/step - loss: 16.5893 - val_loss: 19.4873\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 17.2716 - val_loss: 19.8571\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 16.4197 - val_loss: 20.8449\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 17.3774 - val_loss: 22.3999\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 1s 29ms/step - loss: 15.7242 - val_loss: 21.0271\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 18.4000 - val_loss: 22.5581\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 17.0205 - val_loss: 21.6660\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 1s 28ms/step - loss: 20.2971 - val_loss: 20.8650\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 18.1022 - val_loss: 22.0236\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 1s 30ms/step - loss: 21.0630 - val_loss: 51.0584\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 1s 42ms/step - loss: 22.3605 - val_loss: 28.5993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a57baa3b6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# train the model\n",
        "model.fit(\n",
        "    [trainAttrX, trainImagesX], trainY,\n",
        "    validation_data=([testAttrX, testImagesX], testY),\n",
        "    epochs=200, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xrQ4ywCjXlQa",
        "outputId": "61fb7bda-b811-4b9e-cf2c-4eb329ec7d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 24ms/step\n"
          ]
        }
      ],
      "source": [
        "# make predictions on the testing data\n",
        "preds = model.predict([testAttrX, testImagesX])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "o3BKPBi_XlQa",
        "outputId": "6b5fcf30-1ba4-4935-a220-acb7082c0b73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] avg. house price: $533,388.27, std house price: $493,403.08\n",
            "[INFO] mean: 28.60%, std: 17.49%\n"
          ]
        }
      ],
      "source": [
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"[INFO] avg. house price: {}, std house price: {}\".format(\n",
        "    locale.currency(df[\"price\"].mean(), grouping=True),\n",
        "    locale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbCXgwFYXlQb"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\"> دوره مقدماتی یادگیری عمیق<br>علیرضا اخوان پور<br>پنج شنبه، ۲۵ بهمن ۱۳۹۷<br>\n",
        "</div>\n",
        "<a href=\"http://class.vision\">Class.Vision</a> - <a href=\"http://AkhavanPour.ir\">AkhavanPour.ir</a> - <a href=\"https://github.com/Alireza-Akhavan/\">GitHub</a>\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}